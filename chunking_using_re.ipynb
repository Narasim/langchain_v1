{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75248a-cc78-405b-a552-f93b94baeaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "# -----------------------------\n",
    "# Sample larger text for demos\n",
    "# -----------------------------\n",
    "sample_text = (\n",
    "    \"\"\"\n",
    "# Introduction\n",
    "\n",
    "In 2024, enterprises accelerated the adoption of retrieval-augmented generation (RAG) to unlock knowledge embedded in sprawling document collections. \n",
    "However, real-world content is messy: PDFs mix tables and prose, wikis interleave code blocks with narratives, and emails include replies quoted in-line. \n",
    "Robust chunking strategies are essential to make downstream retrieval both accurate and efficient.\n",
    "\n",
    "This guide surveys foundational chunking approaches, explains trade-offs, and shows how to implement them. \n",
    "We illustrate with a fictional case study of a product team documenting a cross-platform release. \n",
    "The goal is to choose chunk boundaries that preserve meaning while enabling fast indexing.\n",
    "\n",
    "## Background\n",
    "\n",
    "RAG systems attach embeddings to chunks. Embeddings are most effective when chunks are semantically coherent and not too long for the model's context window. \n",
    "Choosing chunk sizes is a balancing act: larger chunks capture more context but risk dilution; smaller chunks increase recall but may fragment meaning.\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "Token: a unit used by language models; not the same as a word. Context window: the max tokens a model can process at once. Overlap: repeated text between adjacent chunks to preserve continuity.\n",
    "\n",
    "# Chapter 1 — Requirements\n",
    "\n",
    "The product targets desktop and mobile. The primary user stories include reading, annotating, and sharing rich documents. \n",
    "Accessibility must meet WCAG criteria. Localization covers five languages: English, Hindi, Telugu, Spanish, and German. \n",
    "Performance requirements specify cold-start under two seconds on mid-tier devices.\n",
    "\n",
    "Paragraph one elaborates on security: the system enforces least-privilege, audits admin actions, and encrypts data at rest and in transit. \n",
    "Paragraph two explains telemetry: we record opt-in usage metrics, minimize personally identifiable information (PII), and provide dashboards.\n",
    "\n",
    "The final paragraph enumerates integration points: identity (SSO), storage (cloud + on-prem), and collaboration (comments, mentions, notifications).\n",
    "\n",
    "# Chapter 2 — Architecture\n",
    "\n",
    "We adopt a modular architecture. The client layer handles UI and offline caching; the sync layer reconciles edits; the service layer persists data. \n",
    "An event bus connects modules via publish-subscribe.\n",
    "\n",
    "Design decisions: we prefer eventual consistency where strict ordering is costly. \n",
    "We use idempotent operations so retries are safe. \n",
    "We separate read paths from write paths to optimize latencies.\n",
    "\n",
    "Failures are expected: we budget for partial outages and degrade gracefully. \n",
    "Monitoring includes traces, metrics, and logs. \n",
    "We practice chaos testing to discover hidden coupling.\n",
    "\n",
    "## Data Model\n",
    "\n",
    "Documents contain sections, paragraphs, and blocks (text, table, image). \n",
    "Each block references its parent and maintains version history. \n",
    "Indexes store derived views for search.\n",
    "\n",
    "# Appendix — Release Notes\n",
    "\n",
    "v1.0.0 (Jan 2025): Initial release with editor, comments, and export to PDF. \n",
    "v1.1.0 (Mar 2025): Added offline mode, improved accessibility, and telemetry opt-in. \n",
    "v1.2.0 (Jun 2025): Introduced collaboration mentions and richer notifications.\n",
    "\n",
    "Known issues: large tables render slowly on older devices. \n",
    "Workarounds: switch to compact table view, or split tables into multiple blocks.\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d340e-9c24-4e1f-a336-21800ac8b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd88595-c35a-4859-8925-45ed10c5f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Approximate tokens: words, numbers, and punctuation as separate tokens.\n",
    "    This is NOT model-accurate but good enough for demonstration without external deps.\n",
    "    \"\"\"\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
    "\n",
    "def tokens_to_text(tokens: List[str]) -> str:\n",
    "    \"\"\"Reconstruct text from tokens naively with space joining rules.\"\"\"\n",
    "    out = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if re.match(r\"^\\w+$\", tok):\n",
    "            # word/number\n",
    "            if i > 0 and re.match(r\"^\\w+$\", tokens[i-1]):\n",
    "                out.append(\" \")\n",
    "            out.append(tok)\n",
    "        else:\n",
    "            # punctuation\n",
    "            out.append(tok)\n",
    "    return \"\".join(out)\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"A simple sentence splitter based on punctuation. It keeps delimiters.\"\"\"\n",
    "    # Replace newlines with spaces to avoid false splits\n",
    "    normalized = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    parts = re.split(r\"([.!?])\", normalized)\n",
    "    sentences = []\n",
    "    for i in range(0, len(parts)-1, 2):\n",
    "        sent = parts[i].strip()\n",
    "        punct = parts[i+1]\n",
    "        if sent:\n",
    "            sentences.append(sent + punct)\n",
    "    # Handle any trailing part\n",
    "    if len(parts) % 2 == 1 and parts[-1].strip():\n",
    "        sentences.append(parts[-1].strip())\n",
    "    return sentences\n",
    "\n",
    "def split_paragraphs(text: str) -> List[str]:\n",
    "    \"\"\"Split on one or more blank lines.\"\"\"\n",
    "    return [p.strip() for p in re.split(r\"\\n\\s*\\n\", text.strip()) if p.strip()]\n",
    "\n",
    "def split_sections(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Return list of (heading, content) pairs using Markdown-style '#' or 'Chapter' lines.\n",
    "    If no heading is found at the top, we create a default 'Document' section.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    sections = []\n",
    "    current_heading = None\n",
    "    current_lines = []\n",
    "\n",
    "    def push_section():\n",
    "        nonlocal current_heading, current_lines\n",
    "        if current_heading is None and current_lines:\n",
    "            sections.append((\"Document\", \"\\n\".join(current_lines).strip()))\n",
    "        elif current_heading is not None:\n",
    "            sections.append((current_heading.strip(), \"\\n\".join(current_lines).strip()))\n",
    "        current_heading = None\n",
    "        current_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r\"^\\s*#{1,6}\\s+\", line) or re.match(r\"^\\s*(Chapter|CHAPTER)\\b\", line) or re.match(r\"^[A-Z][A-Z\\s\\-]+$\", line.strip()):\n",
    "            # New section boundary\n",
    "            if current_heading is not None or current_lines:\n",
    "                push_section()\n",
    "            current_heading = re.sub(r\"^\\s*#{1,6}\\s+\", \"\", line).strip()\n",
    "        else:\n",
    "            current_lines.append(line)\n",
    "    # Push last\n",
    "    if current_heading is not None or current_lines:\n",
    "        push_section()\n",
    "\n",
    "    return sections\n",
    "\n",
    "# -----------------------------\n",
    "# Chunkers\n",
    "# -----------------------------\n",
    "def chunk_by_fixed_chars(text: str, max_chars: int = 600, overlap: int = 60) -> List[str]:\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        end = min(i + max_chars, n)\n",
    "        chunk = text[i:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == n:\n",
    "            break\n",
    "        i = end - overlap if overlap > 0 else end\n",
    "        i = max(i, 0)\n",
    "    return chunks\n",
    "\n",
    "def chunk_by_fixed_tokens(text: str, max_tokens: int = 200, overlap: int = 20) -> List[str]:\n",
    "    tokens = simple_tokenize(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(tokens)\n",
    "    while i < n:\n",
    "        end = min(i + max_tokens, n)\n",
    "        chunk_tokens = tokens[i:end]\n",
    "        chunk_text = tokens_to_text(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        if end == n:\n",
    "            break\n",
    "        i = end - overlap if overlap > 0 else end\n",
    "        i = max(i, 0)\n",
    "    return chunks\n",
    "\n",
    "def chunk_by_sentences(text: str, max_sentences: int = 5, overlap: int = 1) -> List[str]:\n",
    "    sents = split_sentences(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(sents)\n",
    "    while i < n:\n",
    "        end = min(i + max_sentences, n)\n",
    "        chunk = \" \".join(sents[i:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == n:\n",
    "            break\n",
    "        i = end - overlap if overlap > 0 else end\n",
    "    return chunks\n",
    "\n",
    "def chunk_by_paragraphs(text: str, max_paragraphs: int = 2, overlap: int = 0) -> List[str]:\n",
    "    paras = split_paragraphs(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(paras)\n",
    "    while i < n:\n",
    "        end = min(i + max_paragraphs, n)\n",
    "        chunk = \"\\n\\n\".join(paras[i:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == n:\n",
    "            break\n",
    "        i = end - overlap if overlap > 0 else end\n",
    "    return chunks\n",
    "\n",
    "def chunk_by_sections(text: str, max_chars_per_section: int = 1200, overlap: int = 100) -> List[str]:\n",
    "    sections = split_sections(text)\n",
    "    chunks = []\n",
    "    for heading, content in sections:\n",
    "        section_text = f\"{heading}\\n\\n{content}\".strip()\n",
    "        if len(section_text) <= max_chars_per_section:\n",
    "            chunks.append(section_text)\n",
    "        else:\n",
    "            # further split long sections by chars with overlap\n",
    "            for sub in chunk_by_fixed_chars(section_text, max_chars=max_chars_per_section, overlap=overlap):\n",
    "                chunks.append(sub)\n",
    "    return chunks\n",
    "\n",
    "def chunk_documents(docs: List[str]) -> List[str]:\n",
    "    \"\"\"Each document is its own chunk.\"\"\"\n",
    "    return [doc.strip() for doc in docs if doc and doc.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be23c8-d66c-4c53-8df5-a6f53c67577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Demonstration\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Fixed-Length Chunking (characters) ===\")\n",
    "    char_chunks = chunk_by_fixed_chars(sample_text, max_chars=500, overlap=50)\n",
    "    print(f\"Chunks: {len(char_chunks)}; First chunk length: {len(char_chunks[0])}\")\n",
    "    print(char_chunks[0][:200] + \"...\\n\")\n",
    "\n",
    "    print(\"=== Fixed-Length Chunking (tokens) ===\")\n",
    "    tok_chunks = chunk_by_fixed_tokens(sample_text, max_tokens=120, overlap=20)\n",
    "    print(f\"Chunks: {len(tok_chunks)}; First chunk tokens: {len(simple_tokenize(tok_chunks[0]))}\")\n",
    "    print(tok_chunks[0][:200] + \"...\\n\")\n",
    "\n",
    "    print(\"=== Sentence-Based Chunking ===\")\n",
    "    sent_chunks = chunk_by_sentences(sample_text, max_sentences=6, overlap=2)\n",
    "    print(f\"Chunks: {len(sent_chunks)}; First chunk sentences: {len(split_sentences(sent_chunks[0]))}\")\n",
    "    print(sent_chunks[0][:200] + \"...\\n\")\n",
    "\n",
    "    print(\"=== Paragraph-Based Chunking ===\")\n",
    "    para_chunks = chunk_by_paragraphs(sample_text, max_paragraphs=2, overlap=1)\n",
    "    print(f\"Chunks: {len(para_chunks)}; First chunk paragraphs: {len(split_paragraphs(para_chunks[0]))}\")\n",
    "    print(para_chunks[0][:200] + \"...\\n\")\n",
    "\n",
    "    print(\"=== Section-Based Chunking ===\")\n",
    "    sec_chunks = chunk_by_sections(sample_text, max_chars_per_section=800, overlap=80)\n",
    "    print(f\"Chunks: {len(sec_chunks)}; First chunk starts with: {sec_chunks[0].splitlines()[0]}\")\n",
    "    print(sec_chunks[0][:200] + \"...\\n\")\n",
    "\n",
    "    print(\"=== Document Splitting (each doc as a chunk) ===\")\n",
    "    docs = [sample_text, sample_text.upper()]  # pretend we have two docs\n",
    "    doc_chunks = chunk_documents(docs)\n",
    "    print(f\"Chunks: {len(doc_chunks)}; First chunk size: {len(doc_chunks[0])}\")\n",
    "    print(doc_chunks[0][:200] + \"...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
