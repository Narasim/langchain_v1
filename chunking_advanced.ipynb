{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f6e84-6545-4a79-a996-59d2d2197371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: nlp_chunking_demo.py\n",
    "import re\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# --- NLP libraries ---\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "# Optional: for markdown heading parsing (cleaner section boundaries)\n",
    "from markdown_it import MarkdownIt\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load a HF tokenizer (choose any model; gpt2 is fine for demo)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# -----------------------------\n",
    "# Sample larger text for demos\n",
    "# -----------------------------\n",
    "SAMPLE_TEXT = \"\"\"\n",
    "# Introduction\n",
    "\n",
    "In 2024, enterprises accelerated the adoption of retrieval-augmented generation (RAG) to unlock knowledge embedded in sprawling document collections. \n",
    "However, real-world content is messy: PDFs mix tables and prose, wikis interleave code blocks with narratives, and emails include replies quoted in-line. \n",
    "Robust chunking strategies are essential to make downstream retrieval both accurate and efficient.\n",
    "\n",
    "This guide surveys foundational chunking approaches, explains trade-offs, and shows how to implement them. \n",
    "We illustrate with a fictional case study of a product team documenting a cross-platform release. \n",
    "The goal is to choose chunk boundaries that preserve meaning while enabling fast indexing.\n",
    "\n",
    "## Background\n",
    "\n",
    "RAG systems attach embeddings to chunks. Embeddings are most effective when chunks are semantically coherent and not too long for the model's context window. \n",
    "Choosing chunk sizes is a balancing act: larger chunks capture more context but risk dilution; smaller chunks increase recall but may fragment meaning.\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "Token: a unit used by language models; not the same as a word. Context window: the max tokens a model can process at once. Overlap: repeated text between adjacent chunks to preserve continuity.\n",
    "\n",
    "# Chapter 1 — Requirements\n",
    "\n",
    "The product targets desktop and mobile. The primary user stories include reading, annotating, and sharing rich documents. \n",
    "Accessibility must meet WCAG criteria. Localization covers five languages: English, Hindi, Telugu, Spanish, and German. \n",
    "Performance requirements specify cold-start under two seconds on mid-tier devices.\n",
    "\n",
    "Paragraph one elaborates on security: the system enforces least-privilege, audits admin actions, and encrypts data at rest and in transit. \n",
    "Paragraph two explains telemetry: we record opt-in usage metrics, minimize personally identifiable information (PII), and provide dashboards.\n",
    "\n",
    "The final paragraph enumerates integration points: identity (SSO), storage (cloud + on-prem), and collaboration (comments, mentions, notifications).\n",
    "\n",
    "# Chapter 2 — Architecture\n",
    "\n",
    "We adopt a modular architecture. The client layer handles UI and offline caching; the sync layer reconciles edits; the service layer persists data. \n",
    "An event bus connects modules via publish-subscribe.\n",
    "\n",
    "Design decisions: we prefer eventual consistency where strict ordering is costly. \n",
    "We use idempotent operations so retries are safe. \n",
    "We separate read paths from write paths to optimize latencies.\n",
    "\n",
    "Failures are expected: we budget for partial outages and degrade gracefully. \n",
    "Monitoring includes traces, metrics, and logs. \n",
    "We practice chaos testing to discover hidden coupling.\n",
    "\n",
    "## Data Model\n",
    "\n",
    "Documents contain sections, paragraphs, and blocks (text, table, image). \n",
    "Each block references its parent and maintains version history. \n",
    "Indexes store derived views for search.\n",
    "\n",
    "# Appendix — Release Notes\n",
    "\n",
    "v1.0.0 (Jan 2025): Initial release with editor, comments, and export to PDF. \n",
    "v1.1.0 (Mar 2025): Added offline mode, improved accessibility, and telemetry opt-in. \n",
    "v1.2.0 (Jun 2025): Introduced collaboration mentions and richer notifications.\n",
    "\n",
    "Known issues: large tables render slowly on older devices. \n",
    "Workarounds: switch to compact table view, or split tables into multiple blocks.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def add_overlap_windows(items: List[str], max_items: int, overlap: int) -> List[List[str]]:\n",
    "    \"\"\"Window items with overlap. Returns a list of lists.\"\"\"\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(items)\n",
    "    while i < n:\n",
    "        end = min(i + max_items, n)\n",
    "        window = items[i:end]\n",
    "        chunks.append(window)\n",
    "        if end == n:\n",
    "            break\n",
    "        i = end - overlap if overlap > 0 else end\n",
    "    return chunks\n",
    "\n",
    "def preview(text: str, length: int = 220) -> str:\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    return (text[:length] + \"...\") if len(text) > length else text\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Fixed-Length Chunking (characters)\n",
    "# -----------------------------\n",
    "def chunk_by_fixed_chars(text: str, max_chars: int = 800, overlap: int = 80) -> List[str]:\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        end = min(i + max_chars, n)\n",
    "        chunk = text[i:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == n: break\n",
    "        i = end - overlap if overlap > 0 else end\n",
    "        i = max(i, 0)\n",
    "    return chunks\n",
    "\n",
    "# -----------------------------\n",
    "# 1b) Fixed-Length Chunking (tokens via HF tokenizer)\n",
    "# -----------------------------\n",
    "def chunk_by_fixed_tokens_hf(text: str, max_tokens: int = 350, overlap: int = 50) -> List[str]:\n",
    "    ids = hf_tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(ids)\n",
    "    while i < n:\n",
    "        end = min(i + max_tokens, n)\n",
    "        window_ids = ids[i:end]\n",
    "        chunk = hf_tokenizer.decode(window_ids)\n",
    "        chunks.append(chunk)\n",
    "        if end == n: break\n",
    "        i = end - overlap if overlap > 0 else end\n",
    "    return chunks\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Sentence-Based Chunking (spaCy or NLTK)\n",
    "# -----------------------------\n",
    "def sentences_spacy(text: str) -> List[str]:\n",
    "    doc = nlp(text)\n",
    "    return [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "\n",
    "def sentences_nltk(text: str) -> List[str]:\n",
    "    # Normalize newlines to avoid weird splits\n",
    "    normalized = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    return [s.strip() for s in sent_tokenize(normalized) if s.strip()]\n",
    "\n",
    "def chunk_by_sentences(text: str, max_sentences: int = 6, overlap: int = 2, use_spacy: bool = True) -> List[str]:\n",
    "    sents = sentences_spacy(text) if use_spacy else sentences_nltk(text)\n",
    "    windows = add_overlap_windows(sents, max_sentences, overlap)\n",
    "    return [\" \".join(w) for w in windows]\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Paragraph-Based Chunking\n",
    "# (Structure split + optional token capping inside)\n",
    "# -----------------------------\n",
    "def split_paragraphs(text: str) -> List[str]:\n",
    "    # Split on one or more blank lines; keep paragraphs intact\n",
    "    return [p.strip() for p in re.split(r\"\\n\\s*\\n\", text.strip()) if p.strip()]\n",
    "\n",
    "def chunk_by_paragraphs(text: str, max_paragraphs: int = 2, overlap: int = 1, max_tokens_cap: Optional[int] = None) -> List[str]:\n",
    "    paras = split_paragraphs(text)\n",
    "    windows = add_overlap_windows(paras, max_paragraphs, overlap)\n",
    "    chunks = [\"\\n\\n\".join(w) for w in windows]\n",
    "    # Optional: cap by token size using HF tokenizer\n",
    "    if max_tokens_cap:\n",
    "        capped = []\n",
    "        for ch in chunks:\n",
    "            if len(hf_tokenizer.encode(ch)) <= max_tokens_cap:\n",
    "                capped.append(ch)\n",
    "            else:\n",
    "                # If too long, sub-split by tokens\n",
    "                capped.extend(chunk_by_fixed_tokens_hf(ch, max_tokens=max_tokens_cap, overlap=max(0, max_tokens_cap // 10)))\n",
    "        return capped\n",
    "    return chunks\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Section-Based Chunking (Markdown headings)\n",
    "# -----------------------------\n",
    "def split_sections_markdown(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse headings using markdown-it-py. Returns list of (heading, content).\n",
    "    Non-heading prefix content goes under 'Document'.\n",
    "    \"\"\"\n",
    "    md = MarkdownIt()\n",
    "    tokens = md.parse(text)\n",
    "    sections = []\n",
    "    current_heading = None\n",
    "    current_content_lines = []\n",
    "\n",
    "    def push_section():\n",
    "        nonlocal current_heading, current_content_lines\n",
    "        content = \"\\n\".join(current_content_lines).strip()\n",
    "        if current_heading is None and content:\n",
    "            sections.append((\"Document\", content))\n",
    "        elif current_heading is not None:\n",
    "            sections.append((current_heading, content))\n",
    "        current_heading = None\n",
    "        current_content_lines = []\n",
    "\n",
    "    i = 0\n",
    "    # Build a flat text from tokens with heading boundaries\n",
    "    while i < len(tokens):\n",
    "        tok = tokens[i]\n",
    "        if tok.type == \"heading_open\":\n",
    "            # Push previous section\n",
    "            if current_heading is not None or current_content_lines:\n",
    "                push_section()\n",
    "            # Next token is heading text\n",
    "            j = i + 1\n",
    "            heading_text = \"\"\n",
    "            while j < len(tokens) and tokens[j].type != \"heading_close\":\n",
    "                if tokens[j].type == \"inline\":\n",
    "                    heading_text = tokens[j].content.strip()\n",
    "                j += 1\n",
    "            current_heading = heading_text\n",
    "            i = j  # skip to heading_close\n",
    "        else:\n",
    "            # Accumulate inline text/token content\n",
    "            if tok.type == \"inline\":\n",
    "                current_content_lines.append(tok.content)\n",
    "            elif tok.type == \"paragraph_open\":\n",
    "                # Collect full paragraph content\n",
    "                j = i + 1\n",
    "                ptext = []\n",
    "                while j < len(tokens) and tokens[j].type != \"paragraph_close\":\n",
    "                    if tokens[j].type == \"inline\":\n",
    "                        ptext.append(tokens[j].content)\n",
    "                    j += 1\n",
    "                if ptext:\n",
    "                    current_content_lines.append(\" \".join(ptext))\n",
    "                i = j\n",
    "            # Other block tokens (e.g., list items) can be similarly handled\n",
    "        i += 1\n",
    "\n",
    "    # Push the trailing section\n",
    "    if current_heading is not None or current_content_lines:\n",
    "        push_section()\n",
    "\n",
    "    return sections\n",
    "\n",
    "def chunk_by_sections(text: str, max_tokens_per_section: int = 400, overlap_tokens: int = 50) -> List[str]:\n",
    "    sections = split_sections_markdown(text)\n",
    "    chunks = []\n",
    "    for heading, content in sections:\n",
    "        section_text = f\"{heading}\\n\\n{content}\".strip()\n",
    "        # If too long, sub-chunk by tokens with overlap\n",
    "        if len(hf_tokenizer.encode(section_text)) <= max_tokens_per_section:\n",
    "            chunks.append(section_text)\n",
    "        else:\n",
    "            # Sub-split by tokens\n",
    "            for sub in chunk_by_fixed_tokens_hf(section_text, max_tokens=max_tokens_per_section, overlap=overlap_tokens):\n",
    "                chunks.append(sub)\n",
    "    return chunks\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Document Splitting (each doc as a chunk)\n",
    "# -----------------------------\n",
    "def chunk_documents(docs: List[str]) -> List[str]:\n",
    "    return [d.strip() for d in docs if d and d.strip()]\n",
    "\n",
    "# -----------------------------\n",
    "# Demo main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Fixed-Length Chunking (characters) ===\")\n",
    "    c_char = chunk_by_fixed_chars(SAMPLE_TEXT, max_chars=600, overlap=60)\n",
    "    print(f\"Chunks: {len(c_char)}; Preview: {preview(c_char[0])}\\n\")\n",
    "\n",
    "    print(\"=== Fixed-Length Chunking (HF tokens) ===\")\n",
    "    c_tok = chunk_by_fixed_tokens_hf(SAMPLE_TEXT, max_tokens=300, overlap=40)\n",
    "    print(f\"Chunks: {len(c_tok)}; Preview: {preview(c_tok[0])}\\n\")\n",
    "\n",
    "    print(\"=== Sentence-Based (spaCy) ===\")\n",
    "    c_sent_spacy = chunk_by_sentences(SAMPLE_TEXT, max_sentences=6, overlap=2, use_spacy=True)\n",
    "    print(f\"Chunks: {len(c_sent_spacy)}; Preview: {preview(c_sent_spacy[0])}\\n\")\n",
    "\n",
    "    print(\"=== Sentence-Based (NLTK) ===\")\n",
    "    c_sent_nltk = chunk_by_sentences(SAMPLE_TEXT, max_sentences=6, overlap=2, use_spacy=False)\n",
    "    print(f\"Chunks: {len(c_sent_nltk)}; Preview: {preview(c_sent_nltk[0])}\\n\")\n",
    "\n",
    "    print(\"=== Paragraph-Based (with token cap) ===\")\n",
    "    c_para = chunk_by_paragraphs(SAMPLE_TEXT, max_paragraphs=2, overlap=1, max_tokens_cap=350)\n",
    "    print(f\"Chunks: {len(c_para)}; Preview: {preview(c_para[0])}\\n\")\n",
    "\n",
    "    print(\"=== Section-Based (Markdown headings + HF tokenizer) ===\")\n",
    "    c_sec = chunk_by_sections(SAMPLE_TEXT, max_tokens_per_section=400, overlap_tokens=50)\n",
    "    print(f\"Chunks: {len(c_sec)}; First chunk starts with: {c_sec[0].splitlines()[0]}\")\n",
    "    print(f\"Preview: {preview(c_sec[0])}\\n\")\n",
    "\n",
    "    print(\"=== Document Splitting ===\")\n",
    "    docs = [SAMPLE_TEXT, SAMPLE_TEXT.upper()]\n",
    "    c_docs = chunk_documents(docs)\n",
    "    print(f\"Chunks: {len(c_docs)}; Preview: {preview(c_docs[0])}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
